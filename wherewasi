1/15/2021

final day.. doing a python side cleanup (again)
using postgres notifications for my queue

[web] -- respond to clients, trigger on demand runs, stream logs
[scheduler] -- poll database and send runs to runner
[runner] -- receive events from scheduler, trigger k8s jobs; follow and propagate updates






1/14/2021
big day
* rewrote all the queries and done with hasura
* rewrote the frontend to take the new format of data (no more *runs* just steps, and we calculate runs from steps on the client)
* wrote and removed dependency management

TODO:
* add back in simple dependencies
* handle syncin kubernetes state better
  => when we work on a pod, add it to an in memory set and when we're done remove it
    if the server starts and theres a step in the db but nothing in the set, we know to pick it back up
  => get container statuses ( e.g. err image pull backoff )
  => handle the case where the pod isn't available (right now it messes up logs)

* i dont feel that good about the scheduling process in terms of consistency, idempotency, etc. it can be improved
* it would be nice to visualize:
  - inter step dependencies
  - run/step durations
  - 

1/13/2021:
the ui is getting there

* need to implement choosing a run to look at (see diagram below)
* need to reimplement sending logs, maybe gate the number of logs being
streamed at any time. aiofile is throwing weird errors when I ask for a few
logs in parallel.
* need to implement at least doing the steps in order if not actual daggyness
  * that means steps need IDs and can be identified with something other than an index
    which will make other things way easier
* need to give pod command and pod name fixed width, and put some space between steps

later on:
  did a big old cleanup!!!!

in the middle of rewriting my logging logger (pod_log.py)
i think some gating on the client side is a good idea (close a request to start a new one for logs)


1/12/2021:
  need to implement logging with automatic rotation
  
  [x] in the middle of implementing a new view page for a flow.

  TIME FOR BED
    => in the middle of rewriting the state updating logic using more generic deep merging


-----------------------------------------------------------
<flow name>                                   <run button>

each step:
  <step name, image, and command>

each run:
  <bar graph with color as state and height as duration>

each step for selected run (default to most recent):
  show status and duration as a long horizontal bar
  log output for that step

   ______
  ğŸ­»|      ğŸ­°ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ®€ğŸ­°
|                                ğŸ­µ
|                                ğŸ­µ
|                                ğŸ­µ
|          LOGS                  ğŸ­µ
|                                ğŸ­µ
|                                ğŸ­µ
|                                ğŸ­µ
|________________________________


After that we need to make sure we emit events over the socket for
changes in state and incorporate them into the UI.

And make sure log requesting is all tied up.

And that's a v0.00001 demo.

Next would be:
  Actual DAGs
  Actual scheduler
  Tests?
  Longer running job?
  Using private images?
  Easy linking to logdna / papertrail / whatever?



1/11/2021:
  In the middle of transmogrifying things to use flow_run_steps intead of the flow_run itself for state.
  
  Next will need to make sure the watcher.py does what it should (write logs, track state in the db)
    * also minimize repetitive writes by comparing the state in the db
  
  Then figure out how to expose the state in a live way to the client (either a long streaming GET or websockets)

  